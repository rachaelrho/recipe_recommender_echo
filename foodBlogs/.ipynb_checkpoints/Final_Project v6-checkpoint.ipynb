{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachaelrho/anaconda/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "#html parsing\n",
    "import urllib2\n",
    "from lxml import etree\n",
    "\n",
    "##text pre-processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "##feature extraction + similarity\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate json files and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatJson(file_list):\n",
    "    full = []\n",
    "    with open(\"allRecipes.json\", \"w\") as outfile:\n",
    "        for f in file_list:\n",
    "            with open(f, 'rb') as infile:\n",
    "                file_data = json.load(infile)\n",
    "                full += file_data\n",
    "        json.dump(full, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def openJson(name):\n",
    "    with open(name, 'rb') as infile:\n",
    "        full = json.load(infile)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = [\"fullhelping_items.json\", \"iLoveVegan_items.json\",\"MinimalistBaker_items.json\", \"ohsheglows_items.json\",\n",
    "             \"PaleOMG_items.json\",\"roastedRoot_items.json\",\"ohmyveggies_items.json\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concatJson(file_list)\n",
    "full = openJson('allRecipes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullDF = pd.DataFrame(full)\n",
    "fullDF1 = fullDF[(fullDF.ingredientsList.str.len() != 0)]\n",
    "recipeDF = fullDF1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipeDF.to_pickle('recipeDF_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipeDF = pd.read_pickle('recipeDF_v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Write ingredients to file format for NYT ingredient tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nytIngredientInput(df):\n",
    "    f = open(\"input.txt\", \"a\")\n",
    "    ingredientList = []\n",
    "    for row in range(0,len(df)):\n",
    "        ingredients = df.ingredientsList.iloc[row]\n",
    "        for i in range(0,len(ingredients)):\n",
    "            line = ingredients[i].encode('ascii','ignore')\n",
    "            ingredientList.append([row,i,line])\n",
    "            f.write(line + ' /' + str(row) + ' ' + '\\n')\n",
    "    f.close()   \n",
    "    return ingredientList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingredientList = nytIngredientInput(recipeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredientDF = pd.DataFrame(ingredientList, columns=['recipeIndex','ingredientIndex','recipeText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipeDF.to_pickle('ingredientDF_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipeDF = pd.read_pickle('ingredientDF_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge NYT ingredient tags with ingredientDF\n",
    "<p>(see NYT_CRF_IngredientPhraseTagger.txt for tags) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingredientTags = openJson('results_6-21.json')\n",
    "ingredientTagsDF = pd.DataFrame(ingredientTags)\n",
    "ingredientTagsDF.to_pickle('ingredientTagsDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingredientTaggedDF = pd.concat([ingredientDF,ingredientTagsDF], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mergedIndex(df):\n",
    "\n",
    "    df['mergedrecipeIndex'] = 'NA'\n",
    "\n",
    "    for i in range(0,len(df)): \n",
    "        try:\n",
    "            text = str(df.name.loc[i])\n",
    "            df.name.loc[i] = text.rsplit(' ', 1)[0]\n",
    "            df.mergedrecipeIndex.loc[i] = int(text.rsplit(' ', 1)[1])\n",
    "        except:\n",
    "            pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredientTaggedDF = mergedIndex(ingredientTaggedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredientTaggedDF.to_pickle('ingredientTaggedDF_v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredientTaggedDF = pd.read_pickle('ingredientTaggedDF_v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ingredient Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredientTaggedDF['name'] = ingredientTaggedDF['name'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ingredientTokens(df,taggeddf):\n",
    "    df['ingredientTokens'] = 0\n",
    "    for i in range(0,len(taggeddf.recipeIndex.unique())):\n",
    "        ingredients = taggeddf.name[taggeddf.recipeIndex == i]\n",
    "        df['ingredientTokens'][i] = str(' '.join(ingredients))  \n",
    "    df['ingredientTokens'] = df['ingredientTokens'].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlpTokens(document):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    \n",
    "    tokens = [PorterStemmer().stem(x) for x in tokens]\n",
    "    tokens = [WordNetLemmatizer().lemmatize(x, 'v') for x in tokens]\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [x for x in tokens if x not in stop] \n",
    "    return ','.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachaelrho/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "recipeDF = ingredientTokens(recipeDF,ingredientTaggedDF)\n",
    "recipeDF.loc[:, 'ingredientTokens'] = recipeDF.loc[:, 'ingredientTokens'].apply(nlpTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipeDF.to_pickle('recipeDF_tokens_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipeDF = pd.read_pickle('recipeDF_tokens_v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopList = ('tbsp','From')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfidfVectorizer(stopList,df):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=.2, max_features=1300, min_df=0.01, stop_words= stopList, use_idf=True, ngram_range=(1,3))\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df.ingredientTokens)\n",
    "    terms = sorted([(i,v) for v,i in tfidf_vectorizer.vocabulary_.items()])\n",
    "    vocab = [x[1] for x in terms]\n",
    "    \n",
    "    return tfidf_vectorizer,tfidf_matrix,terms,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer,tfidf_matrix,terms,vocab = tfidfVectorizer(['nan','tbsp','tsp',','],recipeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, Sigma, VT = randomized_svd(tfidf_matrix, n_components=25,\n",
    "                                      n_iter=5,\n",
    "                                      random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.70401137,   9.2286809 ,   6.4956524 ,   6.27837369,\n",
       "         6.09858166,   5.69478327,   5.64946554,   5.48057607,\n",
       "         5.38515481,   5.3163961 ,   5.19272557,   5.03023903,\n",
       "         4.98172928,   4.91637011,   4.85159163,   4.6862408 ,\n",
       "         4.64935523,   4.62623189,   4.52034741,   4.42738316,\n",
       "         4.38788085,   4.29596209,   4.26232211,   4.17663352,   4.12821984])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NMFactorize(tfidf_matrix,n):\n",
    "    NMFmodel = NMF(n_components=n, init='random', random_state=0)\n",
    "    NMF_matrix = NMFmodel.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    return NMFmodel,NMF_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NMFmodel,NMF_matrix = NMFactorize(tfidf_matrix,15) ##chose 15 components since eigen vector largely falls under general food types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity with example input ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topCosine(ingInput, NMF_matrix):\n",
    "    \n",
    "    document = ','.join(ingInput)\n",
    "    testTokens = nlpTokens(document)\n",
    "    tf = tfidf_vectorizer.transform([testTokens])\n",
    "    NMF = NMFmodel.transform(tf)\n",
    "    cosine = cosine_similarity(NMF, NMF_matrix) \n",
    "    cosineList = [(e,i) for i,e in enumerate(list(cosine[0]))] #, reverse = True) ##sorted\n",
    "            \n",
    "    return document,cosineList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topIng(cosineList,recipeDF): ##may need to run this without defined function (was previously malfunctioning)\n",
    "    \n",
    "    recipes = []\n",
    "\n",
    "    for i in range(0,len(cosineList)): \n",
    "        ing = [cosineList[i][0], cosineList[i][1], list(recipeDF.ingredientTokens[i:i+1])]\n",
    "        recipes.append(ing)\n",
    "    \n",
    "    finalList = []\n",
    "\n",
    "    for i in range(0,len(recipes)): \n",
    "        if all(ing in recipes[i][2][0] for ing in ingInput): \n",
    "            finalList.append(recipes[i])\n",
    "    \n",
    "    return recipes,finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingInput = ['chicken','avocado','tomato']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document,cosineList = topCosine(ingInput,NMF_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipes, finalList = topIng(cosineList,recipeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.91685352610868631,\n",
       "  1293,\n",
       "  [u'chicken,breast,yellow,green,bell,jalapeno,nan,coconut,oil,tomato,tomato,sauc,garlic,cumin,chili,powder,oregano,salt,cilantro,avocado']],\n",
       " [0.79922038806708151,\n",
       "  1519,\n",
       "  [u'Grill,Tequila,Lime,Chicken,chicken,breast,oliv,oil,tequila,lime,juic,chili,powder,onion,powder,garlic,powder,sea,salt,Vinaigrett,oliv,oil,tequila,lime,juic,honey,cilantro,For,Salad,mix,green,babi,kale,arugula,avocado,tomato,red,onion,carrot,fresno']],\n",
       " [0.92034505593664095,\n",
       "  1690,\n",
       "  [u'Caesar,Marin,Grill,Chicken,chicken,1690,Tessama,Caesar,Dress,1690,Kale,Caesar,Salad,lacinato,avocado,tomato,goat,chevr,dress']],\n",
       " [0.92339329182205332,\n",
       "  1737,\n",
       "  [u'chicken,bacon,spring,green,mix,tomato,egg,avocado,feta,Mayo,Free,Green,Goddess,Dress']]]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutrDict = {}\n",
    "\n",
    "nutrDict['glutenFree'] = ['wheat','wheatberries','durum','emmer','semolina','spelt',\n",
    "             'farina','farro','graham','rye','barley','tricale','malt']\n",
    "\n",
    "nutrDict['nutFree'] = ['almond','brazil nut','cashew','chestnut','filbert',\n",
    "           'hazelnut','hickory nut','macadamia nut','pecans','pine nut',\n",
    "           'pistachio','walnut']\n",
    "\n",
    "nutrDict['dairyFree'] = ['milk','butter','cream','half & half','sour cream','ghee','yogurt','cheese']\n",
    "\n",
    "nutrDict['vegetarian'] = ['meat','lamb','beef','fish','shrimp','lobster','chicken']\n",
    "\n",
    "nutrDict['vegan'] = ['milk','butter','cream','half & half','sour cream','ghee','yogurt','cheese','egg','meat','lamb']\n",
    "\n",
    "##Low Glycemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nutrPref(pref,finalList):\n",
    "    \n",
    "    delIndex = []\n",
    "    \n",
    "    for i in range(0,len(finalList)):\n",
    "        if any(ing in finalList[i][2][0] for ing in nutrDict[pref]):\n",
    "            delIndex.append(i) ##delIndex.append(i)\n",
    "            \n",
    "    nutrList = [x for i, x in enumerate(finalList) if i not in delIndex]\n",
    "    return nutrList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nutrList = nutrPref('glutenFree',finalList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalization with example bookmarked recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/rachaelrho/ds/metis/final_project/User2_bookmarks.txt') as f:\n",
    "    User2 = []\n",
    "    for line in f:\n",
    "        User2.append(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def userIng(userLinks):\n",
    "\n",
    "    userIngsList = []\n",
    "    \n",
    "    for link in range(0,len(userLinks)):\n",
    "        url = userLinks[link]\n",
    "        response = urllib2.urlopen(url)\n",
    "        htmlparser = etree.HTMLParser()\n",
    "        tree = etree.parse(response, htmlparser)\n",
    "        ings = tree.xpath('//*[contains(@class, \"name\")]/text()')\n",
    "\n",
    "        ingsClean = []\n",
    "\n",
    "        for ing in range(0,len(ings)):\n",
    "            ingClean = ings[ing].rstrip(\" \").rstrip(\"\\n\").lstrip(\"\\n\")\n",
    "            ingsClean.append(ingClean)\n",
    "\n",
    "        userIngsList.append(ingsClean) \n",
    "        \n",
    "    \n",
    "    \n",
    "    return userIngsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user2IngList = userIng(User2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def usertopCos(userIngsList,NMF_matrix,nutrList):\n",
    "    \n",
    "    userdocFull = []\n",
    "    usercosListFull = []\n",
    "    \n",
    "    for i in range(0,len(userIngsList)):\n",
    "        \n",
    "        userdoc,usertopCos = topCosine(userIngsList[i],NMF_matrix)\n",
    "        usercosList = sorted([x for x in usertopCos if x[1] in (x[0] for x in nutrList)])\n",
    "        \n",
    "        userdocFull.append([userdoc])\n",
    "        usercosListFull.append(usercosList)\n",
    "    \n",
    "    return userdocFull,usercosListFull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userdocFull,usercosListFull = usertopCos(user2IngList,NMF_matrix,nutrList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def userfinalList(nutrList,bookmarkList,nutrW,bookmarkW):\n",
    "    \n",
    "    nutrList = sorted([r[0:2] for r in nutrList], key =  lambda x: x[1])\n",
    "    bookmarkList = [sorted(usercosListFull[r], key = lambda tup: tup[1]) for r in range(0,len(usercosListFull))]\n",
    "    userAvg = np.mean(usercosListFull, dtype=np.float64, axis=0)\n",
    "    \n",
    "    userList = []\n",
    "                                                                \n",
    "    for i in range(0,len(nutrList)):\n",
    "        finalW = nutrList[i][0] * nutrW  + userAvg[i][0] * bookmarkW\n",
    "        userList.append((finalW,nutrList[i][1]))\n",
    "        \n",
    "    return userList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userList = userfinalList(nutrList,usercosListFull,0.8,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.85620964053111936, 1293),\n",
       " (0.7495382741747022, 1519),\n",
       " (0.84037637700442991, 1690),\n",
       " (0.84747264382126775, 1737)]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userList ##shows that recipe no. 1293 is best choice for user2 based on preferences, despite third best choice on pure cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##json data load of majority of ingredient combinations and recommended recipes in upcoming notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
